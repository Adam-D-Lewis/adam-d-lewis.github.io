<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2025-07-29T19:57:05+00:00</updated><id>/feed.xml</id><title type="html">Adam Lewis Portfolio</title><subtitle>A website to show off recent projects and articles.</subtitle><entry><title type="html">My Take on DeepLearning.AI‚Äôs Post-Training of LLMs Course</title><link href="/blog/post-training-llms/" rel="alternate" type="text/html" title="My Take on DeepLearning.AI‚Äôs Post-Training of LLMs Course" /><published>2025-06-29T00:00:00+00:00</published><updated>2025-06-29T00:00:00+00:00</updated><id>/blog/post-training-llms</id><content type="html" xml:base="/blog/post-training-llms/"><![CDATA[<p>So I just finished DeepLearning.AI‚Äôs <a href="https://www.deeplearning.ai/short-courses/post-training-of-llms/">Post-Training of LLMs course</a>, and honestly? It was pretty much exactly what I needed‚Äîa straightforward intro to how you actually fine-tune these big language models after they‚Äôve done their initial training.</p>

<h2 id="what-the-course-covers">What the Course Covers</h2>

<p>They break it down into three main ways to do this stuff:</p>

<p><strong>Supervised Fine-Tuning (SFT)</strong> is basically when you want to make big changes to how your model behaves. Want to turn a regular foundation model into something that actually follows instructions? Or maybe teach it to use tools? That‚Äôs SFT territory. The big takeaway here is that quality beats quantity every time‚Äî1,000 really good, diverse examples will crush a million mediocre ones.</p>

<p><strong>Direct Preference Optimization (DPO)</strong> is kind of like showing the model examples of ‚Äúdo this, not that.‚Äù You give it both good and bad responses so it learns what you actually want. This works great for smaller adjustments like making it safer, better at multilingual stuff, or just following instructions better. Pro tip: start with a model that can already answer questions, then use DPO to polish it up.</p>

<p><strong>Online Reinforcement Learning</strong> is where things get really interesting (and complicated). The model generates responses in real-time, gets scored by humans or other models, and then updates itself based on that feedback. Think about how ChatGPT was trained with PPO, or what DeepSeek does with GRPO.</p>

<h2 id="what-i-actually-liked-about-it">What I Actually Liked About It</h2>

<p>The best part? They actually tell you when to use each method instead of just throwing theory at you. You get real advice on how to curate your data, what mistakes to avoid (like when DPO gets obsessed with surface-level patterns), and how much memory each approach is going to eat up.</p>

<p>Plus, they handle all the setup through their Jupyter notebook thing, which is honestly a relief when you just want to learn the concepts without spending half your time fighting with dependencies.</p>

<h2 id="the-not-so-great-parts">The Not-So-Great Parts</h2>

<p>Okay, real talk‚Äîsome of the hands-on stuff felt a bit like when your older sibling lets you ‚Äúplay‚Äù video games but gives you the controller that‚Äôs not actually plugged in. üòÑ You‚Äôre going through the motions, but you‚Äôre not really in control. Still, it gives you a decent foundation if you want to actually implement this stuff yourself later.</p>

<p>Also, this definitely isn‚Äôt for people who are new to LLMs. You should already get the basics of how language models work before jumping into the fine-tuning world.</p>

<p>For me, this course was pretty much perfect for what I needed‚Äîan intro to post-training methods without having to slog through dense academic papers. It‚Äôs short, well-organized, and gives you enough understanding to figure out which rabbit holes are actually worth exploring.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://www.deeplearning.ai/short-courses/post-training-of-llms/">Post-Training of LLMs Course</a> - The main course</li>
  <li><a href="https://github.com/YanCotta/post_training_llms">Additional Code Examples</a> - Extra code that expands on what‚Äôs covered in the course</li>
</ul>]]></content><author><name>Adam Lewis</name></author><category term="open-source" /><summary type="html"><![CDATA[So I just finished DeepLearning.AI‚Äôs Post-Training of LLMs course, and honestly? It was pretty much exactly what I needed‚Äîa straightforward intro to how you actually fine-tune these big language models after they‚Äôve done their initial training. What the Course Covers They break it down into three main ways to do this stuff: Supervised Fine-Tuning (SFT) is basically when you want to make big changes to how your model behaves. Want to turn a regular foundation model into something that actually follows instructions? Or maybe teach it to use tools? That‚Äôs SFT territory. The big takeaway here is that quality beats quantity every time‚Äî1,000 really good, diverse examples will crush a million mediocre ones. Direct Preference Optimization (DPO) is kind of like showing the model examples of ‚Äúdo this, not that.‚Äù You give it both good and bad responses so it learns what you actually want. This works great for smaller adjustments like making it safer, better at multilingual stuff, or just following instructions better. Pro tip: start with a model that can already answer questions, then use DPO to polish it up. Online Reinforcement Learning is where things get really interesting (and complicated). The model generates responses in real-time, gets scored by humans or other models, and then updates itself based on that feedback. Think about how ChatGPT was trained with PPO, or what DeepSeek does with GRPO. What I Actually Liked About It The best part? They actually tell you when to use each method instead of just throwing theory at you. You get real advice on how to curate your data, what mistakes to avoid (like when DPO gets obsessed with surface-level patterns), and how much memory each approach is going to eat up. Plus, they handle all the setup through their Jupyter notebook thing, which is honestly a relief when you just want to learn the concepts without spending half your time fighting with dependencies. The Not-So-Great Parts Okay, real talk‚Äîsome of the hands-on stuff felt a bit like when your older sibling lets you ‚Äúplay‚Äù video games but gives you the controller that‚Äôs not actually plugged in. üòÑ You‚Äôre going through the motions, but you‚Äôre not really in control. Still, it gives you a decent foundation if you want to actually implement this stuff yourself later. Also, this definitely isn‚Äôt for people who are new to LLMs. You should already get the basics of how language models work before jumping into the fine-tuning world. For me, this course was pretty much perfect for what I needed‚Äîan intro to post-training methods without having to slog through dense academic papers. It‚Äôs short, well-organized, and gives you enough understanding to figure out which rabbit holes are actually worth exploring. Resources Post-Training of LLMs Course - The main course Additional Code Examples - Extra code that expands on what‚Äôs covered in the course]]></summary></entry><entry><title type="html">ContainDS Dashboards Integration</title><link href="/blog/cdsdashboard-qhub-onprem/" rel="alternate" type="text/html" title="ContainDS Dashboards Integration" /><published>2021-02-15T00:00:00+00:00</published><updated>2021-02-15T00:00:00+00:00</updated><id>/blog/cdsdashboard-qhub-onprem</id><content type="html" xml:base="/blog/cdsdashboard-qhub-onprem/"><![CDATA[<p><em>The image above is of the <a href="https://github.com/plotly/dash-sample-apps/tree/master/apps/dash-web-trader">Dash Web Trader</a> dashboard.  Dashboards like the one above can now be easily shared in QHub OnPrem via Contain DS Dashboards.</em></p>

<p>Recently, I was able to integrate fantastic open source projects: <a href="https://github.com/Quansight/qhub-onprem">Quansight‚Äôs QHub</a> and <a href="https://cdsdashboards.readthedocs.io/en/stable/">ContainDS Dashboards</a>.</p>

<h1 id="what-problem-does-qhub-onprem-solve">What problem does QHub OnPrem solve?</h1>
<p>QHub OnPrem allows teams to efficiently collaborate by making it easy to share files, environments, scalable infrastructure, and now dashboards in a seamless and secure manner.  Under the hood, Qhub OnPrem is an opinionated open source deployment of <a href="https://jupyterhub.readthedocs.io/en/stable/">JupyterHub</a> with some very useful complementary tools included.  Qhub couples environment management, infrastructure monitoring, and shared infrastructure use with ease of deployment.  While QHub OnPrem is aimed at on-premise infrastructure, <a href="https://github.com/Quansight/qhub-cloud">QHub-Cloud</a> is the cloud equivalent, and can be deployed on any of the major cloud providers.</p>

<p><img src="/assets/images/cdsdashboards/qhub-onprem-architecture.png" alt="QHub OnPrem Architecture" /></p>

<h1 id="what-problem-does-containds-dashboards-solve">What problem does ContainDS Dashboards solve?</h1>
<p>ContainDS Dashboards is an early-stage, publishing solution for Data Scientists to quickly, easily, and securely share results with decision makers.  Like QHub, it‚Äôs also an open source project and when we saw ContainDS Dashboards we knew it was a natural fit for QHub.  ContainDS currently supports Plotly Dash, Panel, Bokeh, Voila, Streamlit, and R Shiny dashboarding libararies.  This allows Data Science Teams to build their apps with whatever tool is most familiar to the Data Scientist or most appropriate for an particular dashboard while maintaining a simple, unified framework for distribution of the various types of dashboards.</p>

<p><img src="/assets/images/cdsdashboards/new-dashboard.png" alt="QHub OnPrem Architecture" style="display:block;margin-left: auto;margin-right: auto;width: 50%;" />
<!-- ![QHub OnPrem Architecture](/assets/images/cdsdashboards/new-dashboard.png) --></p>

<h1 id="integration-of-containds-dashboards-into-qhub-onprem">Integration of ContainDS Dashboards into QHub OnPrem</h1>
<p>ContainDS looked great, but it only integrated with the most standard JupyterHub deployments such as <a href="https://github.com/jupyterhub/the-littlest-jupyterhub">The Littlest Jupyter</a> or <a href="https://github.com/jupyterhub/zero-to-jupyterhub-k8s">Zero to JupyterHub with Kubernetes</a> which scales out user sessions via local processes or Kubernetes respectively.  QHub OnPrem, on the other hand, uses the Slurm Cluster Management software to spin up additional user sessions.</p>

<p>Integrating ContainDS Dashboards into QHub OnPrem involved extending ContainDS Dashboards to support the slurmspawner class in the batchspawner library.  Additionally, I took care to ensure that QHub remained easily configurable, so users who didn‚Äôt wish to use CDS Dashboards could still use QHub without installing it.  With a combination of bash scripting, ansible playbooks, and searching through Jinja Templated HTML, I was able to ensure that CDS Dashboards was fully integrated into QHub OnPrem when users set a simple configuration flag.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Integration of ContainDS Dashboards into QHub]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/cdsdashboards/dash-web-trader-rectangle.png" /><media:content medium="image" url="/assets/images/cdsdashboards/dash-web-trader-rectangle.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Personal Financial Data Scraping with Prefect (Writeup Coming Soon)</title><link href="/blog/data-scraping-pipelines/" rel="alternate" type="text/html" title="Personal Financial Data Scraping with Prefect (Writeup Coming Soon)" /><published>2021-01-15T00:00:00+00:00</published><updated>2021-01-15T00:00:00+00:00</updated><id>/blog/data-scraping-pipelines</id><content type="html" xml:base="/blog/data-scraping-pipelines/"><![CDATA[<p>I recently set up regularly scheduled scraping of Personal Data via Selenium (for scraping) and Docker and Prefect (for running regularly scheduled pipelines in the background).</p>

<p>More details coming soon!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Scraping of Personal Financial Data with Selenium, Docker, and Prefect]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/finscrape/prefect-dashboard.png" /><media:content medium="image" url="/assets/images/finscrape/prefect-dashboard.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Add Azure Support for Open Source Data Science Platform (Writeup Coming Soon)</title><link href="/blog/qhub-cloud-azure/" rel="alternate" type="text/html" title="Add Azure Support for Open Source Data Science Platform (Writeup Coming Soon)" /><published>2021-01-10T00:00:00+00:00</published><updated>2021-01-10T00:00:00+00:00</updated><id>/blog/qhub-cloud-azure</id><content type="html" xml:base="/blog/qhub-cloud-azure/"><![CDATA[<p>Qhub Cloud is an scalable, open source data science platform created by my coworkers and myself at Quansight.  It allows Data Science Teams to easily collaborate as well as deploy large, cost effective Dask clusters on AWS, Google Cloud, or Digital Ocean without needing a DevOps guru.  I recently added support for Azure to Qhub Cloud.</p>

<p>More details coming soon!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Details adding Azure support for Qhub Cloud]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/azure-qhub.png" /><media:content medium="image" url="/assets/images/azure-qhub.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Spatial Filtering at Scale with Dask and Spatialpandas</title><link href="/blog/spatial-filtering/" rel="alternate" type="text/html" title="Spatial Filtering at Scale with Dask and Spatialpandas" /><published>2020-10-14T00:00:00+00:00</published><updated>2020-10-14T00:00:00+00:00</updated><id>/blog/spatial-filtering</id><content type="html" xml:base="/blog/spatial-filtering/"><![CDATA[<p><em>Photo Credit: NASA Earth Observatory images by Joshua Stevens, using Suomi NPP VIIRS data from Miguel Rom√°n, NASA‚Äôs Goddard Space Flight Center</em></p>

<p><em>This article is cross-published on <a href="https://www.quansight.com/post/spatial-filtering-at-scale-with-dask-and-spatialpandas">Quansight‚Äôs blog</a>.</em></p>

<p>Imagine having a dataset of over 50 TB of compressed geospatial point data stored in flat files, and you want to efficiently filter data in a few zip codes for further processing. You can‚Äôt even open a dataset that large on a single machine using tools like pandas, so what is the best way to accomplish the filtering? This is exactly the problem one of our clients recently faced.</p>

<p>We addressed this challenge by spatially sorting the data, storing it in a partitionable binary file format, and parallelizing spatial filtering of the data all while using only open source tools within the PyData ecosystem on a commercial cloud platform. This white paper documents the potential solutions that we considered to address our client‚Äôs needs.</p>

<h2 id="potential-approaches">Potential Approaches</h2>
<p>Our client was a small startup who needed to avoid large up-front infrastructure costs. This constrained our approaches to those possible via cloud providers like AWS, Azure, or GCP. We considered five potential approaches to meet the client‚Äôs needs, one of which employs a relational database, and four that use the PyData stack.</p>

<h2 id="relational-database-with-geospatial-extension">Relational Database with Geospatial Extension</h2>
<p>Perhaps the first approach that comes to mind is the use of a relational database such as PostgreSQL and an extension like PostGIS which allows the use of spatial data types and queries on AWS Relational Database Service (RDS). The advantage of this approach is that it‚Äôs well established, but the strong disadvantage is cost.</p>

<p><img src="/assets/images/spatial_filter_post/aws_rds_costs_table.jpg" alt="" /></p>

<p>Databases hosted on AWS RDS have a variety of costs, but in this case, with such a large amount of data, database storage costs dominate. Using RDS requires using more expensive database storage rather than using S3 storage. Table 1 compares the costs of RDS Database Storage and S3 Storage at the time of writing.</p>

<p>Database storage is 5x the cost of S3 storage making an RDS database approach unattractive. Approaches that allow the data to be accessed directly from S3 are more cost effective. This cost difference led us to consider the PyData stack.</p>

<h2 id="pydata-stack">PyData Stack</h2>

<p>In this approach, building a solution using open-source libraries is more cost effective and transparent, but it‚Äôs more than that for us at Quansight. We are experts in the PyData open source tool stack. Many core contributors and creators of popular python open source packages have found a home at Quansight. We often see solutions that make significant use of open source tools, so naturally, this is where we focused the remainder of our development.</p>

<p>Various open source python packages exist which could be combined to accomplish spatial sorting. We built several solutions and compared their performances below. Generally, each solution consists of five components, which are illustrated in Figure 2.</p>

<p><img src="/assets/images/spatial_filter_post/fig2.png" alt="" />
<em>Figure 2: Illustration of the general workflow for solutions based on the PyData stack approach.</em></p>

<p>The five components are:</p>

<ol>
  <li>Use a partitionable binary file format</li>
  <li>Spatially sort the dataset</li>
  <li>Create a global data index</li>
  <li>Parallelize data access and processing</li>
  <li>Spatially filter the relevant partitions</li>
</ol>

<p>We chose these components generally to reduce memory use and increase filtering speed. The first component, a partitionable binary file format, was useful because partitions allow for subsets of the data to be read. The second component was spatial sorting. It is the process of mapping 2-dimensional data to a single dimension such that points near each other in the single dimension are near each other in the original 2-dimensions, and then sorting the data by that single dimension. The third component, creating a global index of the partitions, allows for efficient lookup of the partitions which contain data in a particular region. After the data was sorted and indexed, it was saved in partitions, which could be opened individually instead of opening the entire dataset at once. With the data in that format, filtering (by zip code polygons in this paper) was much faster because we only needed to open the data partitions relevant to a specific region. The fourth component was parallelized access and processing of the relevant partitions. The relevant partitions of the dataset include points both within and outside the set of zip code polygons. The fifth component, another filtering step, was necessary to exclude data outside the zip code polygons.</p>

<h2 id="spatial-sorting">Spatial Sorting</h2>

<p>Spatial sorting deserves more explanation, and it can be performed using a variety of techniques. The idea is to map the original point data from latitude-longitude space to a 1-dimensional space such that points that are near each other in the 1-dimensional space are also near each other in latitude-longitude space. We can then sort the points based on the 1-dimensional space value. If that isn‚Äôt clear, the coming examples describing geohash and Hilbert curve spatial sorting should help.</p>

<p>Several established systems could be used for spatially sorting the data including Geohash, Hilbert Curve, Uber H3, and Google S2 Geometry. We only considered the first two methods due to time constraints, but they are conceptually similar to the last two methods.</p>

<h3 id="geohash-spatial-sort-example">Geohash Spatial Sort Example</h3>

<p>Using geohashes is the first way we considered spatially sorting the point data, but what is a geohash? In practical terms, a geohash is a string of characters which represents a rectangular region in latitude-longitude space. In Figure 3, geohash ‚Äú9‚Äù represents the area contained in the red rectangle. Smaller and smaller subregions can be defined within the larger geohash by adding more characters. You can see that the geohash ‚Äú9q‚Äù represents the region in the light blue rectangle which is contained within geohash ‚Äú9‚Äù. You can continue adding characters to the geohash to define an arbitrarily small subregion.</p>

<p><img src="/assets/images/spatial_filter_post/geohash_with_inset.png" alt="" />
<em>Figure 3: Background: Select one-character geohash regions plotted over a world map.  Inset: All two-character geohashes starting with ‚Äú9‚Äù as the first character plotted over a map.</em></p>

<p>To spatially sort the data with geohashes, we mapped each point to its four character geohash, and then sorted the geohashes lexicographically. This results in the 2D latitude-longitude space being mapped to a 1D geohash space (string of characters) which can be sorted. The power of this method depended on the fact that points near each other in geohash space are also near each other in latitude-longitude space.</p>

<h3 id="hilbert-curve-spatial-sort-example">Hilbert Curve Spatial Sort Example</h3>

<p>Instead of using geohashes, the point data could be spatially sorted via a Hilbert curve. In Figure 4, the Hilbert curve (red), is a continuous curve beginning in the bottom left, and ending in the bottom right which fills the 2D latitude-longitude space. If we snap our point data, to the nearest part of the Hilbert curve, we can define each point by the distance it lies along the Hilbert curve. We can then sort the data by the 1D Hilbert curve distance. After doing so, we‚Äôve mapped our 2D latitude-longitude data to a 1D Hilbert curve distance dimension, and points which are near each other on the Hilbert curve are also near each other in latitude-longitude space.</p>

<div style="text-align: center">
<img src="/assets/images/spatial_filter_post/fig4.png" width="400" />
</div>

<p><em>Figure 4: Image of 2D points plotted over a Hilbert curve in a latitude-longitude coordinate system. The red curvy line is the Hilbert curve inside the box. The points are arbitrary examples showing where they might lie relative to the curve.</em></p>

<h2 id="tech-stacks-used-in-pydata-solutions">Tech stacks used in PyData solutions</h2>

<p>Now that the components of the general approach have been explained, let‚Äôs get into the specific packages implemented in the four solutions we tested. We conducted performance tests on the following stacks (Table 2) to help determine the best solution for our client.</p>

<p>Table 2: Packages used in each Spatial Filtering Solution
<img src="/assets/images/spatial_filter_post/Tech_Stack_Table_Image.png" alt="" /></p>

<p>Parquet was used in all four potential solutions as the binary file format allowing partitioning of the data for subsequently accessing only relevant data partitions. For spatial sorting, solutions using both geohashes via Python-Geohash and the Hilbert curve via Spatialpandas were considered. Dask was used to build a global index of the data in the geohash sorted solutions, while Spatialpandas built the global index of the data in the Hilbert curve solution. Dask is used to read the data in parallel in all cases. Additionally, Dask is used to map the spatial filtering function across each of the data partitions in all cases. The spatial filtering function, a spatial join (explained below), was part of GeoPandas in the first 2 cases, and part of Spatialpandas in the last case. In the Sorted Geohash No Sjoin case, no final spatial filtering was performed, resulting in lower accuracy solution than the other cases.</p>

<p>The spatial filtering function used in the last step was a spatial join. A spatial join is like a regular database join, except the keys being joined are geometric objects and the relationships between them can include geometric relationships (e.g. Join by points within a polygon). Different spatial join methods are implemented in GeoPandas and Spatialpandas.</p>

<h2 id="benchmark">Benchmark</h2>

<p>In order to compare the various solutions, we established a benchmark against which to compare the solution performances in terms of runtime. The task is to filter a large amount of point data by various randomly selected US zip code polygons. We performed the task five times for each solution, each time increasing the number of zip code polygons. The dataset and machine specification details are given below. Our work can be freely downloaded and reproduced from it‚Äôs repository: <a href="https://github.com/Quansight/scipy2020_spatial_algorithms_at_scale">https://github.com/Quansight/scipy2020_spatial_algorithms_at_scale</a>.</p>

<p>Each benchmarking test follows these steps:</p>
<ul>
  <li>Preprocess data (if necessary) one time
    <ul>
      <li>Calculate geohash or Hilbert curve</li>
      <li>Spatially sort data</li>
      <li>Save data in partitions</li>
    </ul>
  </li>
  <li>Filter data for each query
    <ul>
      <li>Select points from dataset that are within 1, 10, 100, 1000, 10000 random zip code polygons distributed around the contiguous US</li>
    </ul>
  </li>
</ul>

<h3 id="dataset">Dataset</h3>

<p>We used a dataset from OpenStreetMap (OSM), which originally consisted of 2.9 billion latitude-longitude point pairs. We removed data outside the contiguous US, leaving 114 million rows of point data in a 3.2 GB uncompressed CSV file. We then converted the data to a parquet file format. The data is available at <a href="https://planet.openstreetmap.org/gps/simple-gps-points-120604.csv.xz">https://planet.openstreetmap.org/gps/simple-gps-points-120604.csv.xz</a>. The polygons are randomly selected US zip code polygons available from the US Census at <a href="https://www2.census.gov/geo/tiger/TIGER2019/ZCTA5/tl_2019_us_zcta510.zip">https://www2.census.gov/geo/tiger/TIGER2019/ZCTA5/tl_2019_us_zcta510.zip</a>.</p>

<h3 id="machine-specifications">Machine Specifications</h3>

<p>Although a cloud cluster was used in production, the benchmark results presented here were performed on a workstation. The specs are included below:</p>

<ul>
  <li>Processor: AMD Ryzen Threadripper 2970WX 24-Core Processor</li>
  <li>RAM: 64 GB</li>
  <li>For this comparison Dask Workers were limited to:
    <ul>
      <li>4 Workers</li>
      <li>2 Threads per Worker</li>
      <li>3 GB RAM per Worker</li>
    </ul>
  </li>
</ul>

<p>Note that the final computation brings the filtered data into the main process potentially using more RAM than an individual worker has.</p>

<h2 id="benchmark-results">Benchmark Results</h2>

<h3 id="unsorted-case">Unsorted Case</h3>

<p>As a baseline, we timed an unsorted case first. In this case, preprocessing and sorting of the data was not necessary. We simply opened up every partition of the data, and used GeoPandas to spatially join each partition of points with the set of zip code polygons. We spatially joined the partitions in parallel with Dask. The results are in Table 3.</p>

<p><img src="/assets/images/spatial_filter_post/table_3.jpg" alt="" /></p>

<p>Each row contains the results of filtering the 114 million initial points by a different number of zip code polygons. Because this baseline does not require preprocessing, the Geohash and sort times are each 0 seconds. The query times were all above 40 minutes, and increased with increasing numbers of zip code polygons. The workers didn‚Äôt have enough memory to filter by 1000 or 10,000 zip code polygons.</p>

<h3 id="sorted-geohash-with-sjoin-case">Sorted Geohash with Sjoin Case</h3>

<p>The next case used python-geohash to create geohashes for each point in our OSM dataset. Remember that this means each point in our dataset was labeled with a string of characters representing its location. Then, Dask was used to sort and index the data by the geohash labels. The geohash calculation and sorting times are one-time costs for a given dataset of points. The remaining steps must be performed each time a query is made, and are explained with the help of Figure 6 below.</p>

<p><img src="/assets/images/spatial_filter_post/fig6.jpg" alt="" />
<em>Figure 6: Illustration showing the spatial features involved in querying data in the sorted geohash case.</em></p>

<p>In Figure 6, there is a bright red region representing a zip code area. The light blue and dark blue points are the subset of the points from our OSM dataset that lie within the pink geohashes (two, side-by-side) which intersect the zip code area. Our goal was to efficiently return the light blue points, those that intersect the zip code area. This was accomplished by opening only the data in the geohashes (partitions) intersecting the zip code polygon (light and dark blue points) using the global index of our spatially sorted data, and then applying spatial filtering to leave only the light blue points.</p>

<p>In terms of our PyData stack, we first find the geohash polygons which intersect the zip code with Polygon-geohasher. Then we open our dataset with Dask. Dask uses its global index to open the partitions of our dataset corresponding to the geohash polygons (dark blue points). Then we used GeoPandas to perform a spatial join between the zip code polygon and the geohash points to exclude points outside of the zip code polygon and keep the points of interest (light blue points). The benchmark results are below.</p>

<p><img src="/assets/images/spatial_filter_post/table4.jpg" alt="" /></p>

<p>The one-time costs alone (30 min) are less than the single query time (~40 min) in the unsorted case (above), and this case was able to filter the data in the 1000 zip code polygon task. It‚Äôs important to note that the numbers of filtered data points are identical to the unsorted case, giving us confidence that we selected the same points.</p>

<h3 id="sorted-geohash-no-sjoin-case">Sorted Geohash No Sjoin Case</h3>

<p>As the number of zip code polygons grows, the last spatial join step takes longer and longer. For some applications, the last spatial join step may not be necessary. The lower accuracy of returning all points that are near the zip code polygons, rather than only those within the zip code polygons reduces the query time significantly. In Figure 6, this solution would return all the light and dark blue points within the geohashes intersecting the zip code rather than just those light blue points within the zip code. This solution is not adequate in all cases, but the reduced query time may be worth it in some use cases. For example, this solution may be adequate when the uncertainty of the point data is greater than the size of the geohashes. The results of leaving off the last spatial join step are below.</p>

<p><img src="/assets/images/spatial_filter_post/table5.jpg" alt="" /></p>

<p>In this case, the query times are much lower, especially when the number of zip code polygons is higher, but the number of result points is also much larger, indicative of the lower filtering precision produced by excluding the last spatial join step.</p>

<h3 id="spatialpandas-case">Spatialpandas Case</h3>

<p>The last case used a new package called Spatialpandas. Spatialpandas spatially sorts the data using the Hilbert curve. The results of using Spatialpandas are below. It wasn‚Äôt possible to separate the preprocessing time from the spatial sort time in this case, so they are included together in the sort time column.</p>

<p><img src="/assets/images/spatial_filter_post/table6.jpg" alt="" />
<em>* Geohash time and sort time are combined because they could not be determined separately with Spatialpandas.</em></p>

<p>This case was much faster than the other cases. Additionally, this was the only case that could filter the data by the 10,000 polygon set with full accuracy. This is because the Spatialpandas spatial join requires less memory than the GeoPandas spatial join method.</p>

<h3 id="benchmark-result-comparison">Benchmark Result Comparison</h3>

<p>We‚Äôve looked at the results individually, but the following plots compare the performances of the potential solutions. The first plot below shows the query times vs the number of zip code polygons by which the data was being filtered. Query times don‚Äôt include the one time costs like calculating geohashes, the Hilbert curve, or sorting the data.</p>

<p>As shown in Figure 7, the query time for the unsorted case (red) took the longest at over 40 minutes, and this case was not able to process the two largest batches of zip code polygons. The query time for the sorted geohash without spatial join (yellow) was the fastest for 100 or more polygons, but remember that it wasn‚Äôt as accurate as the other solutions. The fastest query time for an accurate solution is the Spatialpandas case (green).</p>

<p><img src="/assets/images/spatial_filter_post/fig7.jpg" alt="" />
<em>Figure 7: Comparison of filtering time for each batch of zip code polygons.  The dashed line indicates the case in which a spatial join was not used and the results are typically less accurate.</em></p>

<p>Having looked at query times, now let‚Äôs look at the one-time preprocessing times shown in Figure 8. No preprocessing was necessary for the unsorted case, so the preprocessing time was 0 minutes. The two sorted geohash cases had the same preprocessing steps and are identical as a result. In the sorted geohash cases, the geohash calculation took 27 minutes, and the sorting took 3 minutes. By contrast, in the Spatialpandas case the preprocessing and sorting steps took just over 1 minute.</p>

<p><img src="/assets/images/spatial_filter_post/fig8.jpg" alt="" /></p>

<p><em>Figure 8: Preprocessing times for the Sorted Geohash and Spatialpandas solutions. For Spatialpandas, the sort time includes the preprocessing time.</em></p>

<p>Figure 9 shows the extrapolated total time (preprocessing + query time) vs the number of queries when filtering the data by 1, 100, and 10,000 polygons in each query. Query times are extrapolated under the assumption of linear scaling of queries based on the results of the single query time.</p>

<p><img src="/assets/images/spatial_filter_post/fig9.jpg" alt="" />
<em>Figure 9: Extrapolated total time vs number of queries using 1 polygon (left), 100 polygons (middle), and 10,000 polygons (right) in each query.</em></p>

<p>Of the approaches with highest accuracy, the Spatialpandas solution is projected to be fastest in all situations. At times, the Sorted Geohash No Sjoin solution has lower total time than Spatialpandas, but this comes at the potentially high cost of accuracy.</p>

<h2 id="additional-considerations">Additional Considerations</h2>

<p>Spatialpandas was the best overall solution explored here, and was what we ended up using for our client. Spatialpandas is an easy to install, pure python package. It makes heavy use of Numba for highly efficient spatial joins and bounding box queries. It‚Äôs also integrated nicely with Dask and the parquet file format.</p>

<p>However, there are additional considerations to keep in mind. Spatialpandas is a young project with a small community at the moment. The documentation is limited compared with more established libraries, and it has low development activity for the moment. Spatialpandas is good at what it does, but it‚Äôs also limited in functionality. If you need Spatialpandas to do something similar, but slightly different from what is shown here, you may be forced to implement changes to the library yourself. Keep these considerations in mind if you‚Äôre thinking of using Spatialpandas in your application.</p>

<h2 id="other-approaches-considered-aws-athena-omnisci-and-nvidia-rapids">Other Approaches Considered: AWS Athena, Omnisci and NVIDIA RAPIDS</h2>

<p>Other approaches were considered and experimented with. AWS Athena is a serverless interactive query service allowing users to query datasets residing in S3 via SQL queries. Though promising, at the time Athena had file size limitations that made it impractical for use and also did not obviate the need to sort and partition the data spatially to improve access times. The other main area considered was GPU acceleration. Two technologies were explored Omnisci and NVIDIA RAPIDS. These showed some promise but turned out to be too expensive to be practical. Omnisci has excellent GPU accelerated geospatial query performance but requires a fairly beefy GPU instance to run and requires that the data be imported into its internal format which also increases storage costs over S3. The NVIDIA RAPIDS suite has a new tool called CuSpatial but in practice, it is a very early prototype and is not usable in production.</p>

<p><em>This work is heavily based on work originally presented at Scipy 2020, and can be viewed <a href="https://www.youtube.com/watch?v=czesBVUoXvk" title="Spatial Algoirthms at Scale with Spatialpandas">here</a>.</em></p>]]></content><author><name>Adam Lewis</name></author><category term="Spatialpandas" /><category term="Spatial Filtering" /><category term="Dask" /><category term="Pandas" /><summary type="html"><![CDATA[Photo Credit: NASA Earth Observatory images by Joshua Stevens, using Suomi NPP VIIRS data from Miguel Rom√°n, NASA‚Äôs Goddard Space Flight Center This article is cross-published on Quansight‚Äôs blog. Imagine having a dataset of over 50 TB of compressed geospatial point data stored in flat files, and you want to efficiently filter data in a few zip codes for further processing. You can‚Äôt even open a dataset that large on a single machine using tools like pandas, so what is the best way to accomplish the filtering? This is exactly the problem one of our clients recently faced. We addressed this challenge by spatially sorting the data, storing it in a partitionable binary file format, and parallelizing spatial filtering of the data all while using only open source tools within the PyData ecosystem on a commercial cloud platform. This white paper documents the potential solutions that we considered to address our client‚Äôs needs. Potential Approaches Our client was a small startup who needed to avoid large up-front infrastructure costs. This constrained our approaches to those possible via cloud providers like AWS, Azure, or GCP. We considered five potential approaches to meet the client‚Äôs needs, one of which employs a relational database, and four that use the PyData stack. Relational Database with Geospatial Extension Perhaps the first approach that comes to mind is the use of a relational database such as PostgreSQL and an extension like PostGIS which allows the use of spatial data types and queries on AWS Relational Database Service (RDS). The advantage of this approach is that it‚Äôs well established, but the strong disadvantage is cost. Databases hosted on AWS RDS have a variety of costs, but in this case, with such a large amount of data, database storage costs dominate. Using RDS requires using more expensive database storage rather than using S3 storage. Table 1 compares the costs of RDS Database Storage and S3 Storage at the time of writing. Database storage is 5x the cost of S3 storage making an RDS database approach unattractive. Approaches that allow the data to be accessed directly from S3 are more cost effective. This cost difference led us to consider the PyData stack. PyData Stack In this approach, building a solution using open-source libraries is more cost effective and transparent, but it‚Äôs more than that for us at Quansight. We are experts in the PyData open source tool stack. Many core contributors and creators of popular python open source packages have found a home at Quansight. We often see solutions that make significant use of open source tools, so naturally, this is where we focused the remainder of our development. Various open source python packages exist which could be combined to accomplish spatial sorting. We built several solutions and compared their performances below. Generally, each solution consists of five components, which are illustrated in Figure 2. Figure 2: Illustration of the general workflow for solutions based on the PyData stack approach. The five components are: Use a partitionable binary file format Spatially sort the dataset Create a global data index Parallelize data access and processing Spatially filter the relevant partitions We chose these components generally to reduce memory use and increase filtering speed. The first component, a partitionable binary file format, was useful because partitions allow for subsets of the data to be read. The second component was spatial sorting. It is the process of mapping 2-dimensional data to a single dimension such that points near each other in the single dimension are near each other in the original 2-dimensions, and then sorting the data by that single dimension. The third component, creating a global index of the partitions, allows for efficient lookup of the partitions which contain data in a particular region. After the data was sorted and indexed, it was saved in partitions, which could be opened individually instead of opening the entire dataset at once. With the data in that format, filtering (by zip code polygons in this paper) was much faster because we only needed to open the data partitions relevant to a specific region. The fourth component was parallelized access and processing of the relevant partitions. The relevant partitions of the dataset include points both within and outside the set of zip code polygons. The fifth component, another filtering step, was necessary to exclude data outside the zip code polygons. Spatial Sorting Spatial sorting deserves more explanation, and it can be performed using a variety of techniques. The idea is to map the original point data from latitude-longitude space to a 1-dimensional space such that points that are near each other in the 1-dimensional space are also near each other in latitude-longitude space. We can then sort the points based on the 1-dimensional space value. If that isn‚Äôt clear, the coming examples describing geohash and Hilbert curve spatial sorting should help. Several established systems could be used for spatially sorting the data including Geohash, Hilbert Curve, Uber H3, and Google S2 Geometry. We only considered the first two methods due to time constraints, but they are conceptually similar to the last two methods. Geohash Spatial Sort Example Using geohashes is the first way we considered spatially sorting the point data, but what is a geohash? In practical terms, a geohash is a string of characters which represents a rectangular region in latitude-longitude space. In Figure 3, geohash ‚Äú9‚Äù represents the area contained in the red rectangle. Smaller and smaller subregions can be defined within the larger geohash by adding more characters. You can see that the geohash ‚Äú9q‚Äù represents the region in the light blue rectangle which is contained within geohash ‚Äú9‚Äù. You can continue adding characters to the geohash to define an arbitrarily small subregion. Figure 3: Background: Select one-character geohash regions plotted over a world map. Inset: All two-character geohashes starting with ‚Äú9‚Äù as the first character plotted over a map. To spatially sort the data with geohashes, we mapped each point to its four character geohash, and then sorted the geohashes lexicographically. This results in the 2D latitude-longitude space being mapped to a 1D geohash space (string of characters) which can be sorted. The power of this method depended on the fact that points near each other in geohash space are also near each other in latitude-longitude space. Hilbert Curve Spatial Sort Example Instead of using geohashes, the point data could be spatially sorted via a Hilbert curve. In Figure 4, the Hilbert curve (red), is a continuous curve beginning in the bottom left, and ending in the bottom right which fills the 2D latitude-longitude space. If we snap our point data, to the nearest part of the Hilbert curve, we can define each point by the distance it lies along the Hilbert curve. We can then sort the data by the 1D Hilbert curve distance. After doing so, we‚Äôve mapped our 2D latitude-longitude data to a 1D Hilbert curve distance dimension, and points which are near each other on the Hilbert curve are also near each other in latitude-longitude space. Figure 4: Image of 2D points plotted over a Hilbert curve in a latitude-longitude coordinate system. The red curvy line is the Hilbert curve inside the box. The points are arbitrary examples showing where they might lie relative to the curve. Tech stacks used in PyData solutions Now that the components of the general approach have been explained, let‚Äôs get into the specific packages implemented in the four solutions we tested. We conducted performance tests on the following stacks (Table 2) to help determine the best solution for our client. Table 2: Packages used in each Spatial Filtering Solution Parquet was used in all four potential solutions as the binary file format allowing partitioning of the data for subsequently accessing only relevant data partitions. For spatial sorting, solutions using both geohashes via Python-Geohash and the Hilbert curve via Spatialpandas were considered. Dask was used to build a global index of the data in the geohash sorted solutions, while Spatialpandas built the global index of the data in the Hilbert curve solution. Dask is used to read the data in parallel in all cases. Additionally, Dask is used to map the spatial filtering function across each of the data partitions in all cases. The spatial filtering function, a spatial join (explained below), was part of GeoPandas in the first 2 cases, and part of Spatialpandas in the last case. In the Sorted Geohash No Sjoin case, no final spatial filtering was performed, resulting in lower accuracy solution than the other cases. The spatial filtering function used in the last step was a spatial join. A spatial join is like a regular database join, except the keys being joined are geometric objects and the relationships between them can include geometric relationships (e.g. Join by points within a polygon). Different spatial join methods are implemented in GeoPandas and Spatialpandas. Benchmark In order to compare the various solutions, we established a benchmark against which to compare the solution performances in terms of runtime. The task is to filter a large amount of point data by various randomly selected US zip code polygons. We performed the task five times for each solution, each time increasing the number of zip code polygons. The dataset and machine specification details are given below. Our work can be freely downloaded and reproduced from it‚Äôs repository: https://github.com/Quansight/scipy2020_spatial_algorithms_at_scale. Each benchmarking test follows these steps: Preprocess data (if necessary) one time Calculate geohash or Hilbert curve Spatially sort data Save data in partitions Filter data for each query Select points from dataset that are within 1, 10, 100, 1000, 10000 random zip code polygons distributed around the contiguous US Dataset We used a dataset from OpenStreetMap (OSM), which originally consisted of 2.9 billion latitude-longitude point pairs. We removed data outside the contiguous US, leaving 114 million rows of point data in a 3.2 GB uncompressed CSV file. We then converted the data to a parquet file format. The data is available at https://planet.openstreetmap.org/gps/simple-gps-points-120604.csv.xz. The polygons are randomly selected US zip code polygons available from the US Census at https://www2.census.gov/geo/tiger/TIGER2019/ZCTA5/tl_2019_us_zcta510.zip. Machine Specifications Although a cloud cluster was used in production, the benchmark results presented here were performed on a workstation. The specs are included below: Processor: AMD Ryzen Threadripper 2970WX 24-Core Processor RAM: 64 GB For this comparison Dask Workers were limited to: 4 Workers 2 Threads per Worker 3 GB RAM per Worker Note that the final computation brings the filtered data into the main process potentially using more RAM than an individual worker has. Benchmark Results Unsorted Case As a baseline, we timed an unsorted case first. In this case, preprocessing and sorting of the data was not necessary. We simply opened up every partition of the data, and used GeoPandas to spatially join each partition of points with the set of zip code polygons. We spatially joined the partitions in parallel with Dask. The results are in Table 3. Each row contains the results of filtering the 114 million initial points by a different number of zip code polygons. Because this baseline does not require preprocessing, the Geohash and sort times are each 0 seconds. The query times were all above 40 minutes, and increased with increasing numbers of zip code polygons. The workers didn‚Äôt have enough memory to filter by 1000 or 10,000 zip code polygons. Sorted Geohash with Sjoin Case The next case used python-geohash to create geohashes for each point in our OSM dataset. Remember that this means each point in our dataset was labeled with a string of characters representing its location. Then, Dask was used to sort and index the data by the geohash labels. The geohash calculation and sorting times are one-time costs for a given dataset of points. The remaining steps must be performed each time a query is made, and are explained with the help of Figure 6 below. Figure 6: Illustration showing the spatial features involved in querying data in the sorted geohash case. In Figure 6, there is a bright red region representing a zip code area. The light blue and dark blue points are the subset of the points from our OSM dataset that lie within the pink geohashes (two, side-by-side) which intersect the zip code area. Our goal was to efficiently return the light blue points, those that intersect the zip code area. This was accomplished by opening only the data in the geohashes (partitions) intersecting the zip code polygon (light and dark blue points) using the global index of our spatially sorted data, and then applying spatial filtering to leave only the light blue points. In terms of our PyData stack, we first find the geohash polygons which intersect the zip code with Polygon-geohasher. Then we open our dataset with Dask. Dask uses its global index to open the partitions of our dataset corresponding to the geohash polygons (dark blue points). Then we used GeoPandas to perform a spatial join between the zip code polygon and the geohash points to exclude points outside of the zip code polygon and keep the points of interest (light blue points). The benchmark results are below. The one-time costs alone (30 min) are less than the single query time (~40 min) in the unsorted case (above), and this case was able to filter the data in the 1000 zip code polygon task. It‚Äôs important to note that the numbers of filtered data points are identical to the unsorted case, giving us confidence that we selected the same points. Sorted Geohash No Sjoin Case As the number of zip code polygons grows, the last spatial join step takes longer and longer. For some applications, the last spatial join step may not be necessary. The lower accuracy of returning all points that are near the zip code polygons, rather than only those within the zip code polygons reduces the query time significantly. In Figure 6, this solution would return all the light and dark blue points within the geohashes intersecting the zip code rather than just those light blue points within the zip code. This solution is not adequate in all cases, but the reduced query time may be worth it in some use cases. For example, this solution may be adequate when the uncertainty of the point data is greater than the size of the geohashes. The results of leaving off the last spatial join step are below. In this case, the query times are much lower, especially when the number of zip code polygons is higher, but the number of result points is also much larger, indicative of the lower filtering precision produced by excluding the last spatial join step. Spatialpandas Case The last case used a new package called Spatialpandas. Spatialpandas spatially sorts the data using the Hilbert curve. The results of using Spatialpandas are below. It wasn‚Äôt possible to separate the preprocessing time from the spatial sort time in this case, so they are included together in the sort time column. * Geohash time and sort time are combined because they could not be determined separately with Spatialpandas. This case was much faster than the other cases. Additionally, this was the only case that could filter the data by the 10,000 polygon set with full accuracy. This is because the Spatialpandas spatial join requires less memory than the GeoPandas spatial join method. Benchmark Result Comparison We‚Äôve looked at the results individually, but the following plots compare the performances of the potential solutions. The first plot below shows the query times vs the number of zip code polygons by which the data was being filtered. Query times don‚Äôt include the one time costs like calculating geohashes, the Hilbert curve, or sorting the data. As shown in Figure 7, the query time for the unsorted case (red) took the longest at over 40 minutes, and this case was not able to process the two largest batches of zip code polygons. The query time for the sorted geohash without spatial join (yellow) was the fastest for 100 or more polygons, but remember that it wasn‚Äôt as accurate as the other solutions. The fastest query time for an accurate solution is the Spatialpandas case (green). Figure 7: Comparison of filtering time for each batch of zip code polygons. The dashed line indicates the case in which a spatial join was not used and the results are typically less accurate. Having looked at query times, now let‚Äôs look at the one-time preprocessing times shown in Figure 8. No preprocessing was necessary for the unsorted case, so the preprocessing time was 0 minutes. The two sorted geohash cases had the same preprocessing steps and are identical as a result. In the sorted geohash cases, the geohash calculation took 27 minutes, and the sorting took 3 minutes. By contrast, in the Spatialpandas case the preprocessing and sorting steps took just over 1 minute. Figure 8: Preprocessing times for the Sorted Geohash and Spatialpandas solutions. For Spatialpandas, the sort time includes the preprocessing time. Figure 9 shows the extrapolated total time (preprocessing + query time) vs the number of queries when filtering the data by 1, 100, and 10,000 polygons in each query. Query times are extrapolated under the assumption of linear scaling of queries based on the results of the single query time. Figure 9: Extrapolated total time vs number of queries using 1 polygon (left), 100 polygons (middle), and 10,000 polygons (right) in each query. Of the approaches with highest accuracy, the Spatialpandas solution is projected to be fastest in all situations. At times, the Sorted Geohash No Sjoin solution has lower total time than Spatialpandas, but this comes at the potentially high cost of accuracy. Additional Considerations Spatialpandas was the best overall solution explored here, and was what we ended up using for our client. Spatialpandas is an easy to install, pure python package. It makes heavy use of Numba for highly efficient spatial joins and bounding box queries. It‚Äôs also integrated nicely with Dask and the parquet file format. However, there are additional considerations to keep in mind. Spatialpandas is a young project with a small community at the moment. The documentation is limited compared with more established libraries, and it has low development activity for the moment. Spatialpandas is good at what it does, but it‚Äôs also limited in functionality. If you need Spatialpandas to do something similar, but slightly different from what is shown here, you may be forced to implement changes to the library yourself. Keep these considerations in mind if you‚Äôre thinking of using Spatialpandas in your application. Other Approaches Considered: AWS Athena, Omnisci and NVIDIA RAPIDS Other approaches were considered and experimented with. AWS Athena is a serverless interactive query service allowing users to query datasets residing in S3 via SQL queries. Though promising, at the time Athena had file size limitations that made it impractical for use and also did not obviate the need to sort and partition the data spatially to improve access times. The other main area considered was GPU acceleration. Two technologies were explored Omnisci and NVIDIA RAPIDS. These showed some promise but turned out to be too expensive to be practical. Omnisci has excellent GPU accelerated geospatial query performance but requires a fairly beefy GPU instance to run and requires that the data be imported into its internal format which also increases storage costs over S3. The NVIDIA RAPIDS suite has a new tool called CuSpatial but in practice, it is a very early prototype and is not usable in production. This work is heavily based on work originally presented at Scipy 2020, and can be viewed here.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/spatial_filter_post/us_at_night.jpg" /><media:content medium="image" url="/assets/images/spatial_filter_post/us_at_night.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Mortgage Calculator App &amp;amp; Panel Tutorial (Live Deployment)</title><link href="/blog/mortgage-calc-app/" rel="alternate" type="text/html" title="Mortgage Calculator App &amp;amp; Panel Tutorial (Live Deployment)" /><published>2020-09-15T00:00:00+00:00</published><updated>2020-09-15T00:00:00+00:00</updated><id>/blog/mortgage-calc-app</id><content type="html" xml:base="/blog/mortgage-calc-app/"><![CDATA[<h2 id="mortgage-calculator">Mortgage Calculator</h2>

<p>Recently, my wife and I were looking at mortgages in preparation to buy a home, and I decided to create a mortgage calculator web app in Python using Panel.  Panel is a nice library that doesn‚Äôt require the data scientist to understand a lot of low level CSS and HTML in order to build a nice looking dashboard or app.</p>

<p>Try out a live demo of the app <a href="https://mybinder.org/v2/gh/adam-d-lewis/panel-learning-aid/master?urlpath=/apps/mortgage_app.ipynb" target="_blank">here</a> (takes a minute or two to spin up), and see the source code for this app <a href="https://github.com/Quansight/panel-learning-aid" target="_blank">here</a>.</p>

<h2 id="panel-tutorial">Panel Tutorial</h2>

<p>After creating the app, I also created a quick 10 minute learning-aid to get someone started with Panel.</p>

<p>Try out a live demo of the learning-aid <a href="https://mybinder.org/v2/gh/adam-d-lewis/panel-learning-aid/master?filepath=learning_aid.ipynb" target="_blank">here</a> (takes a minute or two to spin up).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Mortgage Payment Calculation Panel App]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/mortgage_calculator/mortgage_calculator_plot3.png" /><media:content medium="image" url="/assets/images/mortgage_calculator/mortgage_calculator_plot3.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Getting Started in Open Source: Lessons learned from a first-time open source contributor</title><link href="/blog/getting-started-in-os/" rel="alternate" type="text/html" title="Getting Started in Open Source: Lessons learned from a first-time open source contributor" /><published>2020-05-22T00:00:00+00:00</published><updated>2020-05-22T00:00:00+00:00</updated><id>/blog/getting-started-in-os</id><content type="html" xml:base="/blog/getting-started-in-os/"><![CDATA[<p><em>This article is cross-published on <a href="https://medium.com/@adam.dudley.lewis/getting-started-in-open-source-software-70cf9e785686">Medium</a>.</em></p>

<p>Being relatively new to the process of contributing to Open Source Software (OSS), I thought it would be helpful to others who are considering becoming contributors to share my experiences. This article will provide insight on why you might want to do contribute, what you might expect the process to be like, and how you can make your first contribution.</p>

<h2 id="why-contribute">Why Contribute?</h2>
<p>Contributing to open source is rewarding in a variety of ways. For many, there is a sense of satisfaction in creating something useful, and with sharing it with others. But there are less altruistic reasons to contribute to OSS as well, and the motivation doesn‚Äôt diminish the value of the contribution. Fixing a bug or adding a feature that would benefit a library you are currently using can be a way to add functionality that you need, while also providing value to the broader community. Additionally, contributing to OSS can signal your level of expertise with a library. With regular, significant contributions over time, you can establish yourself as an expert with a library, and perhaps become a part of the formal governance structure as a core developer, maintainer, etc. All or nearly all major companies rely on open source software, and many of them employ developers of open source projects they care about.</p>

<h2 id="determining-where-and-what-to-contribute">Determining Where and What to Contribute</h2>
<p>For those seeking to contribute to open source for the first time, there are a few strategies which can help you get the most out of contributing. First, choose a well-established project that you already use. By choosing a package that you already use, you‚Äôll jump start your ability to contribute. Learning a new code base is not trivial, and the insight from having used a library previously is valuable when jumping in for the first time. By choosing a library you‚Äôre familiar with, you‚Äôll be able to contribute more effectively with less effort. You‚Äôll also improve your mastery of a library that you already use.</p>

<p>Contributing to well established projects as a first-time contributor is useful as well-established projects tend to have better documentation on how and where to contribute. These documents make it easier for you to be effective. Search around for something along the lines of ‚ÄúDevelopment Guidelines‚Äù within the project‚Äôs website and/or repository. This documentation will contain code style standards, channels of communication to communicate with the project‚Äôs other developers, best practices, etc. If you want your contribution to get accepted, you‚Äôll need to follow these guidelines.</p>

<p>As far as determining what to contribute, if you have a feature you‚Äôd like to add or bug you‚Äôd like to fix, then open an issue, and get some feedback from the maintainers. For feature requests, other developers will help determine if the feature is within the scope of the project, and possibly give guidance on how it might be implemented. If you aren‚Äôt sure what you‚Äôd like to commit, check out the project‚Äôs issues. This is a list of identified fixes and enhancements that the community has identified. Many well-established projects have specific labels such as ‚Äúgood first issue‚Äù which have been identified by the core developers as an approachable issue to start with.</p>

<h2 id="making-the-contribution">Making the Contribution</h2>
<p>When you‚Äôve found what you‚Äôd like to contribute, the next steps are to fork the repository, make your changes, and then submit a pull request. This article doesn‚Äôt cover the mechanics of those steps. If you haven‚Äôt done them before, then a quick Google or YouTube search will fill you in. There a few things to keep in mind as you make your contribution.</p>

<h2 id="make-an-attempt-before-requesting-too-much-feedback-on-your-implementation">Make an attempt before requesting too much feedback on your implementation</h2>
<p>It can seem easier to ask many questions of the core developers before starting on your contribution. Some questions can be useful initially, but it‚Äôs usually better to jump in and actually attempt to work on the issue at hand before seeking too much feedback. Once you‚Äôve coded at least a proof of concept addressing the issue, you can submit a PR and seek feedback. The core developers and maintainers are busy, and I‚Äôve found they‚Äôre much more willing to give feedback once you‚Äôve shown willingness to invest some time into creating and submitting something. It‚Äôs also much easier for them to critique code than it is to critique an written explanation of how you plan to proceed. Making and submitting your proof of concept attempt before seeking too much feedback will help ensure you get useful feedback.</p>

<h2 id="its-going-to-take-longer-than-youexpect">It‚Äôs going to take longer than you¬†expect</h2>
<p>Making your first contribution to open source is almost certainly going to take longer than you expect, and there are a few reasons for that. First, even if the user facing documentation is complete, the internal documentation is often much less so. This can make it difficult to understand the implementation details that you might need to modify or use for your contribution. The debugger in a good IDE is your friend here for particularly tricky sections of code, but don‚Äôt get too bogged down. While understanding all the implementation details is ideal, it‚Äôs often sufficient to merely understand internal functions at a high level. Additionally, you may need to learn some of the libraries the library you are contributing to depends on. This can take some time if you aren‚Äôt familiar with them.</p>

<p>Finally, you‚Äôll likely need to make several rounds of edits as you get feedback from the core developers. Although it might seem clear how to proceed, there may exist a better way to implement what you‚Äôve done. You may have missed an edge case that wasn‚Äôt apparent to you. Whatever the reason, you‚Äôll likely need major edits for your first contribution. However, you‚Äôll also improve your understanding with each edit, and produce a higher quality result as well.</p>

<h2 id="be-courteous">Be Courteous</h2>
<p>Finally, be courteous as you go through this process. Remember that many of the core developers are volunteers. Being rude is not going to help anyone get what they want. It may take a few days for the core developers to review your pull request. If you‚Äôve waited a few days and haven‚Äôt heard anything, politely reach out again and request a review so that your efforts don‚Äôt go to waste. In my limited experience, courtesy has been the norm. Help keep it that way.</p>

<h2 id="keep-going">Keep Going</h2>
<p>After your pull request is successfully merged, congratulations! You‚Äôve given back to the community, made a valuable contribution, and learned something in the process. After you‚Äôve overcome this hurdle, keep going. Know that your next contribution will be easier as you leverage what you‚Äôve learned. With consistent effort, you can continue to make valuable contributions, increase your overall skill, and improve your mastery of that library.</p>]]></content><author><name>Adam Lewis</name></author><category term="open-source" /><summary type="html"><![CDATA[This article is cross-published on Medium. Being relatively new to the process of contributing to Open Source Software (OSS), I thought it would be helpful to others who are considering becoming contributors to share my experiences. This article will provide insight on why you might want to do contribute, what you might expect the process to be like, and how you can make your first contribution. Why Contribute? Contributing to open source is rewarding in a variety of ways. For many, there is a sense of satisfaction in creating something useful, and with sharing it with others. But there are less altruistic reasons to contribute to OSS as well, and the motivation doesn‚Äôt diminish the value of the contribution. Fixing a bug or adding a feature that would benefit a library you are currently using can be a way to add functionality that you need, while also providing value to the broader community. Additionally, contributing to OSS can signal your level of expertise with a library. With regular, significant contributions over time, you can establish yourself as an expert with a library, and perhaps become a part of the formal governance structure as a core developer, maintainer, etc. All or nearly all major companies rely on open source software, and many of them employ developers of open source projects they care about. Determining Where and What to Contribute For those seeking to contribute to open source for the first time, there are a few strategies which can help you get the most out of contributing. First, choose a well-established project that you already use. By choosing a package that you already use, you‚Äôll jump start your ability to contribute. Learning a new code base is not trivial, and the insight from having used a library previously is valuable when jumping in for the first time. By choosing a library you‚Äôre familiar with, you‚Äôll be able to contribute more effectively with less effort. You‚Äôll also improve your mastery of a library that you already use. Contributing to well established projects as a first-time contributor is useful as well-established projects tend to have better documentation on how and where to contribute. These documents make it easier for you to be effective. Search around for something along the lines of ‚ÄúDevelopment Guidelines‚Äù within the project‚Äôs website and/or repository. This documentation will contain code style standards, channels of communication to communicate with the project‚Äôs other developers, best practices, etc. If you want your contribution to get accepted, you‚Äôll need to follow these guidelines. As far as determining what to contribute, if you have a feature you‚Äôd like to add or bug you‚Äôd like to fix, then open an issue, and get some feedback from the maintainers. For feature requests, other developers will help determine if the feature is within the scope of the project, and possibly give guidance on how it might be implemented. If you aren‚Äôt sure what you‚Äôd like to commit, check out the project‚Äôs issues. This is a list of identified fixes and enhancements that the community has identified. Many well-established projects have specific labels such as ‚Äúgood first issue‚Äù which have been identified by the core developers as an approachable issue to start with. Making the Contribution When you‚Äôve found what you‚Äôd like to contribute, the next steps are to fork the repository, make your changes, and then submit a pull request. This article doesn‚Äôt cover the mechanics of those steps. If you haven‚Äôt done them before, then a quick Google or YouTube search will fill you in. There a few things to keep in mind as you make your contribution. Make an attempt before requesting too much feedback on your implementation It can seem easier to ask many questions of the core developers before starting on your contribution. Some questions can be useful initially, but it‚Äôs usually better to jump in and actually attempt to work on the issue at hand before seeking too much feedback. Once you‚Äôve coded at least a proof of concept addressing the issue, you can submit a PR and seek feedback. The core developers and maintainers are busy, and I‚Äôve found they‚Äôre much more willing to give feedback once you‚Äôve shown willingness to invest some time into creating and submitting something. It‚Äôs also much easier for them to critique code than it is to critique an written explanation of how you plan to proceed. Making and submitting your proof of concept attempt before seeking too much feedback will help ensure you get useful feedback. It‚Äôs going to take longer than you¬†expect Making your first contribution to open source is almost certainly going to take longer than you expect, and there are a few reasons for that. First, even if the user facing documentation is complete, the internal documentation is often much less so. This can make it difficult to understand the implementation details that you might need to modify or use for your contribution. The debugger in a good IDE is your friend here for particularly tricky sections of code, but don‚Äôt get too bogged down. While understanding all the implementation details is ideal, it‚Äôs often sufficient to merely understand internal functions at a high level. Additionally, you may need to learn some of the libraries the library you are contributing to depends on. This can take some time if you aren‚Äôt familiar with them. Finally, you‚Äôll likely need to make several rounds of edits as you get feedback from the core developers. Although it might seem clear how to proceed, there may exist a better way to implement what you‚Äôve done. You may have missed an edge case that wasn‚Äôt apparent to you. Whatever the reason, you‚Äôll likely need major edits for your first contribution. However, you‚Äôll also improve your understanding with each edit, and produce a higher quality result as well. Be Courteous Finally, be courteous as you go through this process. Remember that many of the core developers are volunteers. Being rude is not going to help anyone get what they want. It may take a few days for the core developers to review your pull request. If you‚Äôve waited a few days and haven‚Äôt heard anything, politely reach out again and request a review so that your efforts don‚Äôt go to waste. In my limited experience, courtesy has been the norm. Help keep it that way. Keep Going After your pull request is successfully merged, congratulations! You‚Äôve given back to the community, made a valuable contribution, and learned something in the process. After you‚Äôve overcome this hurdle, keep going. Know that your next contribution will be easier as you leverage what you‚Äôve learned. With consistent effort, you can continue to make valuable contributions, increase your overall skill, and improve your mastery of that library.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/assets/images/computer.jpg" /><media:content medium="image" url="/assets/images/computer.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>