<!-- This layout is used in all pages. Making changes here will affect all pages. We recommend not to change anything here. --> <!DOCTYPE html><html lang="en"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" /><link rel="dns-prefetch" href="//fonts.googleapis.com" /><link rel="dns-prefetch" href="//google-analytics.com" /><link rel="dns-prefetch" href="//www.google-analytics.com" /><link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com" /><link rel="dns-prefetch" href="//ajax.googleapis.com" /><link rel="dns-prefetch" href="//fonts.gstatic.com" /><link rel="dns-prefetch" href="https://portfolio-ghw33nbcja.disqus.com/" /><title>My Take on DeepLearning.AI‚Äôs Post-Training of LLMs Course | Adam Lewis Portfolio</title><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="My Take on DeepLearning.AI‚Äôs Post-Training of LLMs Course" /><meta name="author" content="Adam Lewis" /><meta property="og:locale" content="en_US" /><meta name="description" content="So I just finished DeepLearning.AI‚Äôs Post-Training of LLMs course, and honestly? It was pretty much exactly what I needed‚Äîa straightforward intro to how you actually fine-tune these big language models after they‚Äôve done their initial training. What the Course Covers They break it down into three main ways to do this stuff: Supervised Fine-Tuning (SFT) is basically when you want to make big changes to how your model behaves. Want to turn a regular foundation model into something that actually follows instructions? Or maybe teach it to use tools? That‚Äôs SFT territory. The big takeaway here is that quality beats quantity every time‚Äî1,000 really good, diverse examples will crush a million mediocre ones. Direct Preference Optimization (DPO) is kind of like showing the model examples of ‚Äúdo this, not that.‚Äù You give it both good and bad responses so it learns what you actually want. This works great for smaller adjustments like making it safer, better at multilingual stuff, or just following instructions better. Pro tip: start with a model that can already answer questions, then use DPO to polish it up. Online Reinforcement Learning is where things get really interesting (and complicated). The model generates responses in real-time, gets scored by humans or other models, and then updates itself based on that feedback. Think about how ChatGPT was trained with PPO, or what DeepSeek does with GRPO. What I Actually Liked About It The best part? They actually tell you when to use each method instead of just throwing theory at you. You get real advice on how to curate your data, what mistakes to avoid (like when DPO gets obsessed with surface-level patterns), and how much memory each approach is going to eat up. Plus, they handle all the setup through their Jupyter notebook thing, which is honestly a relief when you just want to learn the concepts without spending half your time fighting with dependencies. The Not-So-Great Parts Okay, real talk‚Äîsome of the hands-on stuff felt a bit like when your older sibling lets you ‚Äúplay‚Äù video games but gives you the controller that‚Äôs not actually plugged in. üòÑ You‚Äôre going through the motions, but you‚Äôre not really in control. Still, it gives you a decent foundation if you want to actually implement this stuff yourself later. Also, this definitely isn‚Äôt for people who are new to LLMs. You should already get the basics of how language models work before jumping into the fine-tuning world. For me, this course was pretty much perfect for what I needed‚Äîan intro to post-training methods without having to slog through dense academic papers. It‚Äôs short, well-organized, and gives you enough understanding to figure out which rabbit holes are actually worth exploring. Resources Post-Training of LLMs Course - The main course Additional Code Examples - Extra code that expands on what‚Äôs covered in the course" /><meta property="og:description" content="So I just finished DeepLearning.AI‚Äôs Post-Training of LLMs course, and honestly? It was pretty much exactly what I needed‚Äîa straightforward intro to how you actually fine-tune these big language models after they‚Äôve done their initial training. What the Course Covers They break it down into three main ways to do this stuff: Supervised Fine-Tuning (SFT) is basically when you want to make big changes to how your model behaves. Want to turn a regular foundation model into something that actually follows instructions? Or maybe teach it to use tools? That‚Äôs SFT territory. The big takeaway here is that quality beats quantity every time‚Äî1,000 really good, diverse examples will crush a million mediocre ones. Direct Preference Optimization (DPO) is kind of like showing the model examples of ‚Äúdo this, not that.‚Äù You give it both good and bad responses so it learns what you actually want. This works great for smaller adjustments like making it safer, better at multilingual stuff, or just following instructions better. Pro tip: start with a model that can already answer questions, then use DPO to polish it up. Online Reinforcement Learning is where things get really interesting (and complicated). The model generates responses in real-time, gets scored by humans or other models, and then updates itself based on that feedback. Think about how ChatGPT was trained with PPO, or what DeepSeek does with GRPO. What I Actually Liked About It The best part? They actually tell you when to use each method instead of just throwing theory at you. You get real advice on how to curate your data, what mistakes to avoid (like when DPO gets obsessed with surface-level patterns), and how much memory each approach is going to eat up. Plus, they handle all the setup through their Jupyter notebook thing, which is honestly a relief when you just want to learn the concepts without spending half your time fighting with dependencies. The Not-So-Great Parts Okay, real talk‚Äîsome of the hands-on stuff felt a bit like when your older sibling lets you ‚Äúplay‚Äù video games but gives you the controller that‚Äôs not actually plugged in. üòÑ You‚Äôre going through the motions, but you‚Äôre not really in control. Still, it gives you a decent foundation if you want to actually implement this stuff yourself later. Also, this definitely isn‚Äôt for people who are new to LLMs. You should already get the basics of how language models work before jumping into the fine-tuning world. For me, this course was pretty much perfect for what I needed‚Äîan intro to post-training methods without having to slog through dense academic papers. It‚Äôs short, well-organized, and gives you enough understanding to figure out which rabbit holes are actually worth exploring. Resources Post-Training of LLMs Course - The main course Additional Code Examples - Extra code that expands on what‚Äôs covered in the course" /><meta property="og:site_name" content="Adam Lewis Portfolio" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-07-29T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="My Take on DeepLearning.AI‚Äôs Post-Training of LLMs Course" /><script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Adam Lewis"},"dateModified":"2025-07-29T00:00:00+00:00","datePublished":"2025-07-29T00:00:00+00:00","description":"So I just finished DeepLearning.AI‚Äôs Post-Training of LLMs course, and honestly? It was pretty much exactly what I needed‚Äîa straightforward intro to how you actually fine-tune these big language models after they‚Äôve done their initial training. What the Course Covers They break it down into three main ways to do this stuff: Supervised Fine-Tuning (SFT) is basically when you want to make big changes to how your model behaves. Want to turn a regular foundation model into something that actually follows instructions? Or maybe teach it to use tools? That‚Äôs SFT territory. The big takeaway here is that quality beats quantity every time‚Äî1,000 really good, diverse examples will crush a million mediocre ones. Direct Preference Optimization (DPO) is kind of like showing the model examples of ‚Äúdo this, not that.‚Äù You give it both good and bad responses so it learns what you actually want. This works great for smaller adjustments like making it safer, better at multilingual stuff, or just following instructions better. Pro tip: start with a model that can already answer questions, then use DPO to polish it up. Online Reinforcement Learning is where things get really interesting (and complicated). The model generates responses in real-time, gets scored by humans or other models, and then updates itself based on that feedback. Think about how ChatGPT was trained with PPO, or what DeepSeek does with GRPO. What I Actually Liked About It The best part? They actually tell you when to use each method instead of just throwing theory at you. You get real advice on how to curate your data, what mistakes to avoid (like when DPO gets obsessed with surface-level patterns), and how much memory each approach is going to eat up. Plus, they handle all the setup through their Jupyter notebook thing, which is honestly a relief when you just want to learn the concepts without spending half your time fighting with dependencies. The Not-So-Great Parts Okay, real talk‚Äîsome of the hands-on stuff felt a bit like when your older sibling lets you ‚Äúplay‚Äù video games but gives you the controller that‚Äôs not actually plugged in. üòÑ You‚Äôre going through the motions, but you‚Äôre not really in control. Still, it gives you a decent foundation if you want to actually implement this stuff yourself later. Also, this definitely isn‚Äôt for people who are new to LLMs. You should already get the basics of how language models work before jumping into the fine-tuning world. For me, this course was pretty much perfect for what I needed‚Äîan intro to post-training methods without having to slog through dense academic papers. It‚Äôs short, well-organized, and gives you enough understanding to figure out which rabbit holes are actually worth exploring. Resources Post-Training of LLMs Course - The main course Additional Code Examples - Extra code that expands on what‚Äôs covered in the course","headline":"My Take on DeepLearning.AI‚Äôs Post-Training of LLMs Course","mainEntityOfPage":{"@type":"WebPage","@id":"/blog/post-training-llms/"},"url":"/blog/post-training-llms/"}</script><link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"><link id="color-scheme" rel="stylesheet" href="/assets/css/main-default.css" /><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/apple-icon-60x60.png" /><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/apple-icon-114x114.png" /><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/apple-icon-152x152.png" /><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/android-icon-192x192.png" /><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" /><link rel="icon" href="/favicon.ico" type="image/x-icon" /><script src="https://cdn.jsdelivr.net/npm/ga-lite@1/dist/ga-lite.min.js" async></script><script> var galite = galite || {}; galite.UA = 'UA-142377027-2';</script><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script><script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script></head><body><div class="loader"><div class="lds-ring"><div></div><div></div><div></div><div></div></div></div><div class="wrapper"><div class="container-grid"><div class="sidebar"><div class="author-container shadow"><div class="author"><img src="/assets/images/author-image.jpg" width="100%" height="auto;" alt="Adam Lewis Portfolio" onclick="location.href='/'"></div><div class="about text-center"><h1 class="title"><a href="/">Adam Lewis</a></h1></div><div class="bio text-center"><p class="m0">Data Scientist | ML Engineer</p></div><hr class="dashed"><div class="social text-center"> <ul class="portfolio p0"> <li><a href="/blog/">Posts</a></li> <li><a href="/contact/">Contact</a></li> </ul> <ul class="sm p0 m0a"> <li><a href="/"><i class="fa fa-home"></i></a></li> <li><a href="https://www.linkedin.com/in/adam-d-lewis/"><i class="fa fa-linkedin"></i></a></li> <li><a href="https://github.com/Adam-D-Lewis"><i class="fa fa-github"></i></a></li> </ul></div></div></div><div class="main"><div class="main-container shadow"><div class="title-space"> <h1>My Take on DeepLearning.AI's Post-Training of LLMs Course</h1><div class="input-group mb-3" data-aos="zoom-in"> <input type="text" class="form-control" id="search-input"><div class="input-group-append"> <span class="input-group-text"><i class="fa fa-search"></i></span></div></div></div><hr class="dashed"> <main> <ul class="breadcrumbs"> <li><a href="/">Home</a></li> <li><a href="/blog/">Blog</a></li> <li><a href="#">Post training llms</a></li> </ul><div class="meta" data-aos="fade-up"> <p> <small> <span> <i class="fa fa-calendar" aria-hidden="true"></i> 29 Jul 2025&nbsp; </span> <span> <i class="fa fa-user" aria-hidden="true"></i> Adam Lewis&nbsp; </span> <span> <i class="fa fa-clock-o" aria-hidden="true"></i> 2 mins read. </span> </small> </p></div><div class="featured-image" style="background-image: url()" data-aos="zoom-in" ></div><div class="container"> <article> <ul class="toc"> <li><a href="#what-the-course-covers">What the Course Covers</a></li> <li><a href="#what-i-actually-liked-about-it">What I Actually Liked About It</a></li> <li><a href="#the-not-so-great-parts">The Not-So-Great Parts</a></li> <li><a href="#resources">Resources</a></li> </ul> <p>So I just finished DeepLearning.AI‚Äôs <a href="https://www.deeplearning.ai/short-courses/post-training-of-llms/">Post-Training of LLMs course</a>, and honestly? It was pretty much exactly what I needed‚Äîa straightforward intro to how you actually fine-tune these big language models after they‚Äôve done their initial training.</p> <h2 id="what-the-course-covers">What the Course Covers</h2> <p>They break it down into three main ways to do this stuff:</p> <p><strong>Supervised Fine-Tuning (SFT)</strong> is basically when you want to make big changes to how your model behaves. Want to turn a regular foundation model into something that actually follows instructions? Or maybe teach it to use tools? That‚Äôs SFT territory. The big takeaway here is that quality beats quantity every time‚Äî1,000 really good, diverse examples will crush a million mediocre ones.</p> <p><strong>Direct Preference Optimization (DPO)</strong> is kind of like showing the model examples of ‚Äúdo this, not that.‚Äù You give it both good and bad responses so it learns what you actually want. This works great for smaller adjustments like making it safer, better at multilingual stuff, or just following instructions better. Pro tip: start with a model that can already answer questions, then use DPO to polish it up.</p> <p><strong>Online Reinforcement Learning</strong> is where things get really interesting (and complicated). The model generates responses in real-time, gets scored by humans or other models, and then updates itself based on that feedback. Think about how ChatGPT was trained with PPO, or what DeepSeek does with GRPO.</p> <h2 id="what-i-actually-liked-about-it">What I Actually Liked About It</h2> <p>The best part? They actually tell you when to use each method instead of just throwing theory at you. You get real advice on how to curate your data, what mistakes to avoid (like when DPO gets obsessed with surface-level patterns), and how much memory each approach is going to eat up.</p> <p>Plus, they handle all the setup through their Jupyter notebook thing, which is honestly a relief when you just want to learn the concepts without spending half your time fighting with dependencies.</p> <h2 id="the-not-so-great-parts">The Not-So-Great Parts</h2> <p>Okay, real talk‚Äîsome of the hands-on stuff felt a bit like when your older sibling lets you ‚Äúplay‚Äù video games but gives you the controller that‚Äôs not actually plugged in. üòÑ You‚Äôre going through the motions, but you‚Äôre not really in control. Still, it gives you a decent foundation if you want to actually implement this stuff yourself later.</p> <p>Also, this definitely isn‚Äôt for people who are new to LLMs. You should already get the basics of how language models work before jumping into the fine-tuning world.</p> <p>For me, this course was pretty much perfect for what I needed‚Äîan intro to post-training methods without having to slog through dense academic papers. It‚Äôs short, well-organized, and gives you enough understanding to figure out which rabbit holes are actually worth exploring.</p> <h2 id="resources">Resources</h2> <ul> <li><a href="https://www.deeplearning.ai/short-courses/post-training-of-llms/">Post-Training of LLMs Course</a> - The main course</li> <li><a href="https://github.com/YanCotta/post_training_llms">Additional Code Examples</a> - Extra code that expands on what‚Äôs covered in the course</li> </ul> </article></div><hr class="dashed" /><div class="container"><div class="row" data-aos="fade-up"><div class="col-md-12"><div class="share-box"> Share with others: <a class="f nostyle" href="https://www.facebook.com/sharer/sharer.php?u=/blog/post-training-llms/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-facebook-official fa"></i ></a> <a class="t nostyle" href="https://twitter.com/intent/tweet?text=&url=/blog/post-training-llms/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-twitter fa"></i ></a> <a class="g nostyle" href="https://plus.google.com/share?url=/blog/post-training-llms/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-google-plus fa"></i ></a> <a class="r nostyle" href="http://www.reddit.com/submit?url=/blog/post-training-llms/" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=900,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-reddit fa"></i ></a> <a class="l nostyle" href="https://www.linkedin.com/shareArticle?mini=true&url=/blog/post-training-llms/&title=&summary=&source=webjeda" onclick="window.open(this.href, 'mywin', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" ><i class="fa fa-linkedin fa"></i ></a> <a class="e nostyle" href="mailto:?subject=&amp;body=Check&amp;out&amp;this&amp;site&amp;/blog/post-training-llms/" ><i class="fa fa-envelope fa"></i ></a></div></div></div><div class="row"><div class="col-md-12"> <p class="categories" data-aos="fade-up"> <span><a href="/categories/#open-source">Open-source</a></span> </p></div></div><div id="disqus_thread" data-aos="fade-up"></div><script defer> (function() { var d = document, s = d.createElement("script"); s.src = "//portfolio-ghw33nbcja.disqus.com/embed.js"; s.setAttribute("data-timestamp", +new Date()); (d.head || d.body).appendChild(s); })();</script><noscript>Please enable JavaScript to view the comments</noscript><div class="recent" data-aos="fade-up"><div class=""> <h3>Recent Posts</h3></div><div class="recent-grid"><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/building-polished-bespoke-solutions-fast-with-vibe-coding/"><div class="cards"><div class="image" style="background-image: url()" ></div><p class="title"><small>Building Polished Bespoke Solutions Fast With Vibe Coding</small></p></div></a></div><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/cdsdashboard-qhub-onprem/"><div class="cards"><div class="image" style="background-image: url(/assets/images/cdsdashboards/dash-web-trader-rectangle.png)" ></div><p class="title"><small>ContainDS Dashboards Integration</small></p></div></a></div><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/data-scraping-pipelines/"><div class="cards"><div class="image" style="background-image: url(/assets/images/finscrape/prefect-dashboard.png)" ></div><p class="title"><small>Personal Financial Data Scraping with Prefect (Writeup Coming Soon)</small></p></div></a></div><div class="items" data-aos="fade-up"> <a class="nostyle item" href="/blog/qhub-cloud-azure/"><div class="cards"><div class="image" style="background-image: url(/assets/images/azure-qhub.png)" ></div><p class="title"><small>Add Azure Support for Open Source Data Science Platform (Writeup Coming Soon)</small></p></div></a></div></div></div></div></main><div id="search-container"></div><hr class="dashed"><footer data-aos="fade-up"><div class="text-right"> <p class="copy"> <i class="fa fa-at"></i> 2020 <a class="rev" href="">Adam Lewis</a> </p></div></footer></div></div></div></div><script src="/assets/js/jQuery.min.js"></script><script> $(document).ready(function () { $(".loader").hide(); });</script><script> (function () { var css = document.createElement('link'); css.href = '/assets/font-awesome-4.7.0/css/font-awesome.min.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="/assets/font-awesome-4.7.0/css/font-awesome.min.css"> </noscript><script> $("#search-input").keyup(function () { $("main").hide(); $("search-container").show(); if (!$('#search-input').val()) { $("main").show(); $("search-container").hide(); } });</script><script src="/assets/js/jekyll-search.min.js" type="text/javascript"></script><script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-container'), searchResultTemplate: '<a class="nostyle" href="{url}"><div class="blog borders cards"><div class="image" style="background-image: url({image});"></div><div class="content"><h3 class="title">{title}</h3><p class="description">{description}</p></div></div></a>', noResultsText: 'No results found', json: '/search.json' })</script><script> (function () { var css = document.createElement('link'); css.href = 'https://unpkg.com/aos@2.3.1/dist/aos.css'; css.rel = 'stylesheet'; css.type = 'text/css'; document.getElementsByTagName('head')[0].appendChild(css); })();</script><noscript><link rel="stylesheet" href="/assets/animate-on-scroll/aos.min.css"> </noscript><script src="/assets/animate-on-scroll/aos.min.js"></script><script> AOS.init({ duration: 600, once: true, disable: 'mobile' });</script></body></html>